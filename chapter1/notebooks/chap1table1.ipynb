{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1, Table 1\n",
    "\n",
    "This notebook explains how I used the Harvard General Inquirer to *streamline* interpretation of a predictive model.\n",
    "\n",
    "I'm italicizing the word \"streamline\" because I want to emphasize that I place very little weight on the Inquirer: as I say in the text, \"The General Inquirer has no special authority, and I have tried not to make it a load-bearing element of this argument.\" \n",
    "\n",
    "To interpret a model, I actually spend a lot of time looking at lists of features, as well as predictions about individual texts. But to *explain* my interpretation, I need some relatively simple summary. Given real-world limits on time and attention, going on about lists of individual words for five pages is rarely an option. So, although wordlists are crude and arbitrary devices, flattening out polysemy and historical change, I am willing to lean on them rhetorically, where I find that they do in practice echo observations I have made in other ways.\n",
    "\n",
    "I should also acknowledge that I'm not using the General Inquirer as it was designed to be used. The full version of this tool is not just a set of wordlists, it's a software package that tries to get around polysemy by disambiguating different word senses. I haven't tried to use it in that way: I think it would complicate my explanation, in order to project an impression of accuracy and precision that I don't particularly want to project. Instead, I have stressed that word lists are crude tools, and I'm using them only as crude approximations.\n",
    "\n",
    "That said, how do I do it?\n",
    "\n",
    "To start with, we'll load an array of modules. Some standard, some utilities that I've written myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some standard modules\n",
    "\n",
    "import csv, os, sys\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# now a module that I wrote myself, located\n",
    "# a few directories up, in the software\n",
    "# library for this repository\n",
    "\n",
    "sys.path.append('../../lib')\n",
    "import FileCabinet as filecab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the General Inquirer.\n",
    "\n",
    "This takes some doing, because the General Inquirer doesn't start out as a set of wordlists. I have to translate it into that form.\n",
    "\n",
    "I start by loading an English dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by loading the dictionary\n",
    "\n",
    "dictionary = set()\n",
    "\n",
    "with open('../../lexicons/MainDictionary.txt', encoding = 'utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter = '\\t')\n",
    "    for row in reader:\n",
    "        word = row[0]\n",
    "        count = int(row[2])\n",
    "        if count < 10000:\n",
    "            continue\n",
    "            # that ignores very rare words\n",
    "            # we end up with about 42,700 common ones\n",
    "        else:\n",
    "            dictionary.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage is to translate the Inquirer. It begins as a table where word senses are row labels, and the Inquirer categories are columns (except for two columns at the beginning and two at the end). This is, by the way, the \"basic spreadsheet\" described at this site:\n",
    "http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm\n",
    "\n",
    "I translate this into a dictionary where the keys are Inquirer categories, and the values are sets of words associated with each category.\n",
    "\n",
    "But to do that, I have to do some filtering and expanding. Different senses of a word are broken out in the spreadsheet thus:\n",
    "\n",
    "ABOUT#1\n",
    "\n",
    "ABOUT#2\n",
    "\n",
    "ABOUT#3\n",
    "\n",
    "etc.\n",
    "\n",
    "I need to separate the hashtag part. Also, because I don't want to allow rare senses of a word too much power, I ignore everything but the first sense of a word.\n",
    "\n",
    "However, I also want to allow singular verb forms and plural nouns to count. So there's some code below that expands words by adding -s -ed, etc to the end. See the *suffixes* dictionary defined below for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inquirer loaded\n",
      "Total of 13707 words.\n"
     ]
    }
   ],
   "source": [
    "inquirer = dict()\n",
    "\n",
    "suffixes = dict()\n",
    "suffixes['verb'] = ['s', 'es', 'ed', 'd', 'ing']\n",
    "suffixes['noun'] = ['s', 'es']\n",
    "\n",
    "allinquirerwords = set()\n",
    "\n",
    "with open('../../lexicons/inquirerbasic.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    fields = reader.fieldnames[2:-2]\n",
    "    for field in fields:\n",
    "        inquirer[field] = set()\n",
    "\n",
    "    for row in reader:\n",
    "        term = row['Entry']\n",
    "\n",
    "        if '#' in term:\n",
    "            parts = term.split('#')\n",
    "            word = parts[0].lower()\n",
    "            sense = int(parts[1].strip('_ '))\n",
    "            partialsense = True\n",
    "        else:\n",
    "            word = term.lower()\n",
    "            sense = 0\n",
    "            partialsense = False\n",
    "\n",
    "        if sense > 1:\n",
    "            continue\n",
    "            # we're ignoring uncommon senses\n",
    "\n",
    "        pos = row['Othtags']\n",
    "        if 'Noun' in pos:\n",
    "            pos = 'noun'\n",
    "        elif 'SUPV' in pos:\n",
    "            pos = 'verb'\n",
    "\n",
    "        forms = {word}\n",
    "        if pos == 'noun' or pos == 'verb':\n",
    "            for suffix in suffixes[pos]:\n",
    "                if word + suffix in dictionary:\n",
    "                    forms.add(word + suffix)\n",
    "                if pos == 'verb' and word.rstrip('e') + suffix in dictionary:\n",
    "                    forms.add(word.rstrip('e') + suffix)\n",
    "\n",
    "        for form in forms:\n",
    "            for field in fields:\n",
    "                if len(row[field]) > 1:\n",
    "                    inquirer[field].add(form)\n",
    "                    allinquirerwords.add(form)\n",
    "                    \n",
    "print('Inquirer loaded')\n",
    "print('Total of ' + str(len(allinquirerwords)) + \" words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model predictions about volumes\n",
    "\n",
    "The next step is to create some vectors that store predictions about volumes. In this case, these are predictions about the probability that a volume is fiction, rather than biography."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have information about 715 volumes.\n"
     ]
    }
   ],
   "source": [
    "# the folder where wordcounts will live\n",
    "# we're only going to load predictions\n",
    "# that correspond to files located there\n",
    "sourcedir = '../sourcefiles/'\n",
    "\n",
    "docs = []\n",
    "logistic = []\n",
    "\n",
    "with open('../plotdata/allsubset2.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        genre = row['realclass']\n",
    "        docid = row['volid']\n",
    "        if not os.path.exists(sourcedir + docid + '.tsv'):\n",
    "            continue\n",
    "        docs.append(row['volid'])\n",
    "        logistic.append(float(row['logistic']))\n",
    "\n",
    "logistic = np.array(logistic)\n",
    "numdocs = len(docs)\n",
    "\n",
    "assert numdocs == len(logistic)\n",
    "\n",
    "print(\"We have information about \" + str(numdocs) + \" volumes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And get the wordcounts themselves\n",
    "\n",
    "This cell of the notebook is very short (one line), but it takes a lot of time to execute. There's a lot of file i/o that happens inside the function get_wordcounts, in the FileCabinet module, which is invoked here. We come away with a dictionary of wordcounts, keyed in the first instance by volume ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcounts = filecab.get_wordcounts(sourcedir, '.tsv', docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now calculate the representation of each Inquirer category in each doc\n",
    "\n",
    "We normalize by the total wordcount for a volume.\n",
    "\n",
    "This cell also takes a long time to run. I've added a counter so you have some confidence that it's still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "101\n",
      "201\n",
      "301\n",
      "401\n",
      "501\n",
      "601\n",
      "701\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty category vectors\n",
    "\n",
    "categories = dict()\n",
    "for field in fields:\n",
    "    categories[field] = np.zeros(numdocs)\n",
    "    \n",
    "# Now fill them\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    ctcat = Counter()\n",
    "    allcats = 0\n",
    "    for word, count in wordcounts[doc].items():\n",
    "        allcats += count\n",
    "        if word not in allinquirerwords:\n",
    "            continue\n",
    "        for field in fields:\n",
    "            if word in inquirer[field]:\n",
    "                ctcat[field] += count\n",
    "    for field in fields:\n",
    "        categories[field][i] = ctcat[field] / (allcats + 0.1)\n",
    "        # Laplacian smoothing there to avoid div by zero, among other things.\n",
    "    \n",
    "    if i % 100 == 1:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate correlations\n",
    "\n",
    "Now that we have all the information, calculating correlations is easy. We iterate through Inquirer categories, in each case calculating the correlation between a vector of model predictions for docs, and a vector of category-frequencies for docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logresults = []\n",
    "\n",
    "for inq_category in fields:\n",
    "    l = pearsonr(logistic, categories[inq_category])[0]\n",
    "    logresults.append((l, inq_category))\n",
    "\n",
    "logresults.sort()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load expanded names of Inquirer categories\n",
    "\n",
    "The terms used in the inquirer spreadsheet are not very transparent. ```DAV``` for instance is \"descriptive action verbs.\" ```BodyPt``` is \"body parts.\" To make these more transparent, I have provided expanded names for many categories that turned out to be relevant in the book, trying to base my description on the accounts provided here: http://www.wjh.harvard.edu/~inquirer/homecat.htm\n",
    "\n",
    "We load these into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short2long = dict()\n",
    "with open('../../lexicons/long_inquirer_names.csv', encoding = 'utf-8') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        short2long[row['short_name']] = row['long_name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print results\n",
    "\n",
    "I print the top 12 correlations and the bottom 12, skipping categories that are drawn from the \"Laswell value dictionary.\" The Laswell categories are very finely discriminated (things like \"enlightenment gain\" or \"power loss\"), and I have little faith that they're meaningful. I especially doubt that they could remain meaningful when the Inquirer is used crudely as a source of wordlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing the correlations of General Inquirer categories\n",
      "with the predicted probabilities of being fiction in allsubset2.csv:\n",
      "\n",
      "First, top positive correlations: \n",
      "\n",
      "0.803277665193\taction verbs\n",
      "0.747302702401\tbody parts\n",
      "0.705410205316\tverbs of sensory perception\n",
      "0.655761134432\tverbs of dialogue\n",
      "0.642988358164\tsecond-person pronouns (likely in dialogue)\n",
      "0.637477149864\tphysical adjectives\n",
      "0.606678113779\tStay\n",
      "0.600470735126\tFemale\n",
      "0.581059270511\tWork\n",
      "0.57749805552\tinterjections and exclamations\n",
      "\n",
      "Now, negative correlations: \n",
      "\n",
      "-0.728900779222\torganized systems of belief or knowledge\n",
      "-0.716711437568\tpolitical terms\n",
      "-0.715712411406\tabstract means\n",
      "-0.680857526622\thuman collectivities\n",
      "-0.676519253746\tpower\n",
      "-0.660826882358\talso power\n",
      "-0.658729939606\tpolitical terms\n",
      "-0.65543434408\teconomic terms\n",
      "-0.596496624471\tName\n"
     ]
    }
   ],
   "source": [
    "print('Printing the correlations of General Inquirer categories')\n",
    "print('with the predicted probabilities of being fiction in allsubset2.csv:')\n",
    "print()\n",
    "print('First, top positive correlations: ')\n",
    "print()\n",
    "for prob, n in reversed(logresults[-12 : ]):\n",
    "    if n in short2long:\n",
    "        n = short2long[n]\n",
    "    if 'Laswell' in n:\n",
    "        continue\n",
    "    else:\n",
    "        print(str(prob) + '\\t' + n)\n",
    "\n",
    "print()\n",
    "print('Now, negative correlations: ')\n",
    "print()\n",
    "for prob, n in logresults[0 : 12]:\n",
    "    if n in short2long:\n",
    "        n = short2long[n]\n",
    "    if 'Laswell' in n:\n",
    "        continue\n",
    "    else:\n",
    "        print(str(prob) + '\\t' + n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "If you compare the printout above to the book's version of Table 1.1, you may notive a few things have been dropped. For instance second-person pronouns aren't specifically mentioned in the table, because I suspect second-person appears mostly in dialogue, and that would add little information to the \"verbs of dialogue\" already mentioned in the table.\n",
    "\n",
    "Titlecased terms are the terms originally used in the Inquirer. Lowercased terms are my explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
